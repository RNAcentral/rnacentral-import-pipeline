#!/usr/bin/env bash

set -euo pipefail
IFS=$'\n\t'

function usage() {
  status="${1:-1}"
  cat <<EOS
fetch <name> [arguments...]

This is the primary script to fetching data. It is what is generally in charge
of fetching data from various places.

names:
  generic url pattern
    This will fetch data at the given url. What this does depends on the type
    of url and the pattern. It can deal with absolute file paths, FTP, HTTP,
    and SSH urls.

  url-file <urls> <pattern>
    Each line in <urls> should be a URL to fetch, the URLs must be http or
    ftp. If <pattern> has a '*' in it each url will be fetched separately,
    otherwise they will be merged into a file called <pattern>. This will not
    decompress any compressed files.

  ensembl url
  ensembl_plants url
    These will fetch the data at the given url and then merge all files from
    the same species into a single <speces>.grouped.gz.

    ex: fetch ensembl ftp://ftp.ensembl.org/pub/current_embl/**/*.dat.gz

  gencode <url-file> <output>
    This assumes the first argument is a url file to fetch. It fetches all data
    in the those files and then extracts the transcript entries from the GFF3 files
    into a single GFF3 file.

EOS
  exit $status
}

function fetch_generic() {
  local remote="$1"
  local pattern="$2"

  case "$remote" in
    ssh://* | scp://* )
      short="${remote:6}"
      scp "$short" .
      ;;

    /* )
      local fn_pattern="$(basename "$remote")"
      if [[ "$fn_pattern" =~ "*" ]]; then
        local dir="$(dirname "$remote")"
        find -L "$dir" -name "$fn_pattern" | xargs -I {} cp {} .
      elif [[ -d "$remote" ]]; then
        cp -r "$remote" .
      else
        cp "$remote" "$pattern"
      fi
      ;;

    http://* | https://* )
      curl "$remote" > "$pattern"
      ;;

    ftp://* )
      # wget doesn't play well with ** so we use lftp instead. Also, wget doesn't
      # like following symlinks, but lftp does. I should find a way to indicate
      # this to the pipeline.
      if [[ "$remote" =~ '**' ]]; then
        lftp -c "mget $remote"
      else
        if [[ "$remote" =~ "*" ]]; then
          wget "$remote"
        elif [[ ! "$pattern" =~ "*" ]]; then
          wget -O "$pattern" "$remote"
        else
          wget "$remote"
        fi
      fi
      ;;

    * )
      echo 1>&2 "Unknown remote URL to fetch: $remote"
      exit 1;;
  esac

  case "$remote" in
    *.tar.gz | *.tgz )
      find . -name '*.tar.gz' -or -name '*.tgz' | xargs -I {} tar xvf {} ;;
  esac
}

function fetch_grouped() {
  local pattern="$(basename "$1")"
  fetch_generic "$1" "$pattern"

  for fn in $pattern; do
    local species="$(echo "$fn" | cut -d. -f1)"
    local grouped="$species.grouped.dat.gz"
    cat "$fn" >> "$grouped"
  done
}

function fetch_quickgo() {
  local remote="$1"
  local output="$2"
  local filename="$(basename "$remote")"

  tmp="$(mktemp)"
  fetch_generic "$remote" "$filename"
  gzip -cd "$filename" > "$tmp"

  {
    head -1 "$tmp"
    grep -v '^!' "$tmp"
  } > "$output"
  rm "$tmp"
}

function fetch_url_file() {
  local remote_file="$1"
  local pattern="$2"
  if [[ "$pattern" =~ "*" ]]; then
    cat "$remote_file" | xargs -I {} wget {}
  else
    cat "$remote_file" | xargs -I {} wget -O - {} >> "$pattern"
  fi
}

function fetch_taxonomy() {
  local remote="$1"
  local output="$2"

  fetch_generic "$1" "*.dmp"
  [ -d "$output" ] || mkdir -p "$output"
  mv *.dmp "$output"
}

function fetch_europepmc() {
  local remote="$1"
  local output="$2"
  local name="$(basename "$remote")"

  fetch_generic "$remote" "$name"
  if [[ ! -d "$output" ]]; then
    mkdir -p "$output"
    mv out/*.xml "$output/"
    rm -r out
  fi
}

function fetch_gencode() {
  local urls="$1"
  local output="$2"
  local tmp="$(mktemp)"

  cat "$urls" |\
  xargs -I {} curl {} |\
  gzip -d - |\
  awk '{ if ($3 == "transcript") print $0 }' > "$output"

  rm "$tmp"
}

function fetch_genome() {
  local remote="$1"
  local genome_file="${2:-genome.fasta}"

  fetch_generic "$remote" '*.fa.gz'
  gzip -d *.fa.gz
  cat *.fa > "$genome_file"
}

function ena_grouper() {
  local to_group="$1"
  local suffix="$2"

  while IFS=',' read -r "filename" "count" || [[ -n "$filename" ]]; do
    local fn="$(basename $filename)"
    local group="${fn:0:$count}$suffix"
    cat $filename >> $group
  done < "$to_group"
}

function fetch_ena() {
  local remote="$1"
  local produces="$2"
  local suffix="${produces:1:${#produces}}"
  local tmp="$(mktemp)"

  find "$remote/con" -name '*.ncr.gz' -type f | xargs -I {} echo {},7 > "$tmp"
  ena_grouper "$tmp" "$suffix"

  find "$remote/std" -name '*.ncr.gz' -not -name 'rel_std_env*.ncr.gz' -type f | xargs -I {} echo {},11 > "$tmp"
  ena_grouper "$tmp" "$suffix"

  # Basically doesn't group the env data as we end up with a single very large
  # file if we do.
  find "$remote/std" -name 'rel_std_env*.ncr.gz' -type f | xargs -I {} echo {},14 > "$tmp"
  ena_grouper "$tmp" "$suffix"

  find "$remote/tsa" -name '*.ncr.gz' -type f | xargs -I {} echo {},3 > "$tmp"
  ena_grouper "$tmp" "$suffix"

  find "$remote/wgs" -name '*.ncr.gz' -type f | xargs -I {} echo {},6 > "$tmp"
  ena_grouper "$tmp" "$suffix"

  rm "$tmp"
}

function fetch_rfam() {
  local remote="$(dirname "$1")"
  local pattern="$(basename "$1")"
  local produces="$2"
  local group_size=9
  local prefix="$(echo $produces | cut -f1 -d *)"
  local suffix="$(echo $produces | cut -f2 -d *)"

  find "$remote" -name "$pattern" |\
  xargs -n $group_size -I {} jq -cs 'flatten' {} |\
  split -edl 1 --additional-suffix "$suffix" - "$prefix"
}

[ $# -gt 0 ] || { usage 0; }

name="$1"
shift 1

case "$name" in
  "ena" )
    fetch_generic "$@" ;;

  "generic" )
    fetch_generic "$@" ;;

  "ensembl" | "ensembl_plants" | "ensembl_fungi" )
    fetch_grouped "$@" ;;

  "europepmc" )
    fetch_europepmc "$@" ;;

  "gencode" )
    fetch_gencode "$@" ;;

  "genome" )
    fetch_genome "$@" ;;

  "rfam" )
    fetch_generic "$@" ;;

  "quickgo" )
    fetch_quickgo "$@" ;;

  "url-file" )
    fetch_url_file "$@" ;;

  "taxonomy" )
    fetch_taxonomy "$@" ;;

  * )
    usage ;;
esac
