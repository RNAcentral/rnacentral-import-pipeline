# -*- coding: utf-8 -*-

"""
Copyright [2009-2021] EMBL-European Bioinformatics Institute
Licensed under the Apache License, Version 2.0 (the "License");
you may not use this file except in compliance with the License.
You may obtain a copy of the License at
http://www.apache.org/licenses/LICENSE-2.0
Unless required by applicable law or agreed to in writing, software
distributed under the License is distributed on an "AS IS" BASIS,
WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
See the License for the specific language governing permissions and
limitations under the License.
"""

import logging
import re
from pathlib import Path

import click
import polars as pl

from rnacentral_pipeline.databases.expressionatlas import configuration, parser, sdrf
from rnacentral_pipeline.databases.expressionatlas.helpers import find_all_taxids
from rnacentral_pipeline.writers import entry_writer

LOGGER = logging.getLogger(__name__)


@click.group("expressionatlas")
def cli():
    """
    Commands for parsing expression atlas data
    """


@cli.command("parse")
@click.argument("genes_mapped", type=click.File("r"))
@click.argument("lookup", type=click.Path())
@click.argument(
    "output",
    default=".",
    type=click.Path(writable=True, dir_okay=True, file_okay=False),
)
def process_csv(genes_mapped, lookup, output):
    """
    Process the csv generated by linking EA data to rnc data

    Args:
        gened_mapped: A concatenated ndjson of all hits for all experiments
        lookup: The retrieved lookup CSV file
        output: The directory in which the load data will be written
    """
    entries = parser.parse(genes_mapped, lookup)

    with entry_writer(Path(output)) as writer:
        try:
            writer.write(entries)
        except ValueError:
            print("No entries from this chunk")


@cli.command("get-taxids")
@click.argument(
    "directory", type=click.Path(exists=True, file_okay=False, dir_okay=True)
)
@click.argument("output")
def get_taxids(directory, output):
    """
    Find all condensed SDRF files in the subdirectory, and parse out all possible taxids.
    Write these out to a file so we can query with them later.

    Args:
        directory: The path to the top-level directory within which are all Expression atlas
            experiments.
        output: A text file that will have one taxid per line, ready to be used in a future
            sql query
    """
    taxids = find_all_taxids(directory)
    with open(output, "w") as out:
        for t in taxids:
            out.write(f"{t}\n")

    LOGGER.info(f"Found {len(taxids)} unique taxids to search for")


@cli.command("parse-dir")
@click.argument(
    "directory", type=click.Path(exists=True, file_okay=False, dir_okay=True)
)
@click.argument("lookup", type=click.Path())
@click.argument("output", type=click.Path())
def parse_dir(directory, lookup, output):
    """
    Look at the files within a directory and parse them appropriately

    This will parse down to a urs_taxid <> experiment mapping, saved as ndjson
    The next parsing stage will create entries using the lookup

    Args:
        directory: The path to a specific experiment that we will try to parse
        lookup: Path to the retrieved lookup CSV file
        output: Output filename for writing the ndjson hits
    """
    directory = Path(directory)
    configurations = list(directory.glob("*configuration.xml"))
    if len(configurations) != 1:
        LOGGER.error("missing configuration for: %s, skipping", directory)
        hits = pl.DataFrame({"urs_taxid": [], "experiment": []})
    else:
        config_file = configurations[0]
        config = configuration.parse_config(config_file)
        if "differential" in config.exp_type:
            try:
                analytics = list(directory.glob("*analytics.tsv"))
                if len(analytics) != 1:
                    raise ValueError(
                        "Didn't find the expected analytics file for %s", directory
                    )
                analytics_file = analytics[0]

                sdrfs = list(directory.glob("*condensed-sdrf.tsv"))
                if len(sdrfs) != 1:
                    raise ValueError("Didn't find the expected SDRF for %s", directory)
                sdrf = sdrfs[0]

                hits = parser.parse_differential(analytics_file, sdrf, lookup)
            except ValueError:
                LOGGER.error("Failed to parse differential experiment %s", directory)
                hits = pl.DataFrame({"urs_taxid": [], "experiment": []})
        elif "baseline" in config.exp_type:
            ## There is a transcripts tpms file which we don't want, so grab both with
            ## pathlib glob, then filter to only get the one we want
            try:
                tpms = list(directory.glob(r"*-tpms.tsv"))
                tpms = list(
                    filter(lambda x: re.match(".*\d+-tpms\.tsv$", str(x)), tpms)
                )
                if len(tpms) != 1:
                    raise ValueError(
                        "Didn't find the expected tpms file for %s", directory
                    )
                tpms = tpms[0]

                sdrfs = list(directory.glob("*condensed-sdrf.tsv"))
                if len(sdrfs) != 1:
                    raise ValueError("Didn't find the expected SDRF for %s", directory)
                sdrf = sdrfs[0]

                hits = parser.parse_baseline(tpms, sdrf, lookup)
            except ValueError:
                hits = pl.DataFrame({"urs_taxid": [], "experiment": []})
                LOGGER.error("failed to parse baseline experiment %s", tpms)
        else:
            LOGGER.error("unknown experiment type: %s", config.exp_type)
            hits = pl.DataFrame({"urs_taxid": [], "experiment": []})

    hits.write_ndjson(output)
